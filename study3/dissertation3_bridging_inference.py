# -*- coding: utf-8 -*-
"""dissertation3_bridging_inference.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RvaC9scTJyEVO0SiODUeYqWac4XcBIiq
"""

!pip install -q openai==0.28
!pip install -q anthropic
!pip install -q google-generativeai
# !pip install -q transformers

### MCQ

import pandas as pd
import time
from tqdm import tqdm
import openai
import anthropic
import google.generativeai as genai


# API 키 설정 (환경변수나 안전한 방식으로 불러올 것)
openai.api_key = ""
claude_client = anthropic.Anthropic(api_key="")
genai.configure(api_key="")


# 데이터 로딩
df = pd.read_csv("mcq.csv")
df.rename(columns={
    df.columns[0]: "id",  # 첫 열이 id라면
    df.columns[1]: "discourse",
    df.columns[2]: "option_a",
    df.columns[3]: "option_b",
    df.columns[4]: "option_c"
}, inplace=True)


# 프롬프트 생성 함수
def make_prompt(discourse, option_a, option_b, option_c):
    return (
        f"Choose the sentence that most naturally follows the given context.\n\n"
        f"Context: {discourse}\n"
        f"Option:\n"
        f"a. {option_a}\n"
        f"b. {option_b}\n"
        f"c. {option_c}\n\n"
        f"Please respond with the option letter only (a, b, or c).\n"
        f"Answer:"
    )


# 모델 질의 함수
def query_gpt(prompt):
    response = openai.ChatCompletion.create(
        model="gpt-4o",
        temperature=0,
        messages=[{"role": "user", "content": prompt}]
    )
    return response['choices'][0]['message']['content']

def query_claude(prompt):
    response = claude_client.messages.create(
        model="claude-3-5-sonnet-20240620",
        max_tokens=30,
        temperature=0,
        messages=[{"role": "user", "content": prompt}]
    )
    return response.content[0].text

def query_gemini(prompt):
    model = genai.GenerativeModel('gemini-1.5-flash')
    response = model.generate_content(prompt)
    return response.text


# 반복 평가
n_runs = 1
results = []

for idx, row in tqdm(df.iterrows(), total=len(df), desc="Evaluating rows"):
    prompt = make_prompt(row["discourse"], row["option_a"], row["option_b"], row["option_c"])

    for model_name, query_func in {
        "gpt": query_gpt,
        "claude": query_claude,
        "gemini": query_gemini
    }.items():
        for run in range(1, n_runs + 1):
            try:
                response = query_func(prompt)
                answer = next((c for c in response.lower() if c in ['a', 'b', 'c']), "")
            except Exception as e:
                print(f"[{model_name}] Error on row {idx}, run {run}: {e}")
                answer = ""

            # 결과 행 추가
            results.append({
                "original_index": idx,
                "run": run,
                "model": model_name,
                "preceding_discourse": row["discourse"],
                "a_mereological": row["option_a"],
                "b_frame": row["option_b"],
                "c_inference": row["option_c"],
                "answer": answer
            })

            time.sleep(0.3)

# DataFrame으로 변환 후 저장
df_results = pd.DataFrame(results)
df_results.to_csv("mcq_result2.csv", index=False)

### acceptability

import pandas as pd
import time
from tqdm import tqdm
import openai
import anthropic
import google.generativeai as genai


# API 키 설정 (환경변수나 안전한 방식으로 불러올 것)
openai.api_key = ""
claude_client = anthropic.Anthropic(api_key="")
genai.configure(api_key="")


# 데이터 로딩
df = pd.read_csv("missing.csv")
# df.rename(columns={
#     df.columns[0]: "id",  # 첫 열이 id라면
#     df.columns[1]: "discourse",
#     df.columns[2]: "sentence",
#     df.columns[3]: "bridging"
# }, inplace=True)


# 프롬프트 생성 함수
def make_prompt(preceding_discourse, target_sentence):
    return (
        f"Read the given context and the following sentence, and rate how naturally the sentence follows the context.\n\n"
        f"Context: {preceding_discourse}\n"
        f"Sentence: {target_sentence}\n"
        f"Please respond with only the number between 1 and 5 (1 = Not natural at all, 5 = Completely natural).\n"
        f"Score:"
    )


# 모델 질의 함수
def query_gpt(prompt):
    response = openai.ChatCompletion.create(
        model="gpt-4o",
        temperature=0,
        messages=[{"role": "user", "content": prompt}]
    )
    return response['choices'][0]['message']['content']

def query_claude(prompt):
    response = claude_client.messages.create(
        model="claude-3-5-sonnet-20240620",
        max_tokens=30,
        temperature=0,
        messages=[{"role": "user", "content": prompt}]
    )
    return response.content[0].text

def query_gemini(prompt):
    model = genai.GenerativeModel('gemini-1.5-flash')
    response = model.generate_content(prompt)
    return response.text


# 반복 평가
n_runs = 1
results = []

for idx, row in tqdm(df.iterrows(), total=len(df), desc="Evaluating rows"):
    prompt = make_prompt(row["preceding_discourse"], row["target_sentence"])

    for model_name, query_func in {
        # "gpt": query_gpt,
        "claude": query_claude
        # "gemini": query_gemini
    }.items():
        for run in range(1, n_runs + 1):
            try:
                response = query_func(prompt)
                score = next((c for c in response if c in ['1', '2', '3', '4', '5']), "")
            except Exception as e:
                print(f"[{model_name}] Error on row {idx}, run {run}: {e}")
                score = ""

            # 결과 행 추가
            results.append({
                "original_index": idx + 1,
                "run": run,
                "model": model_name,
                "preceding_discourse": row["preceding_discourse"],
                "target_sentence": row["target_sentence"],
                "relation_type": row["relation_type"],
                "word_order": row["word_order"],
                "is_type": row["is_type"],
                "score": score
            })

            time.sleep(0.3)

# DataFrame으로 변환 후 저장
df_results = pd.DataFrame(results)
df_results.to_csv("accept_result_re10.csv", index=False)

!huggingface-cli login

from transformers import AutoTokenizer, AutoModelForCausalLM, TextGenerationPipeline, pipeline
import pandas as pd
import time
from tqdm import tqdm
import torch

# LLaMA 3 사전 로딩
model_id = "meta-llama/Meta-Llama-3-8B-Instruct"

tokenizer = AutoTokenizer.from_pretrained(model_id)
llama_model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    device_map="auto",
    low_cpu_mem_usage=True
)

# 테스트용 텍스트 생성 파이프라인
llama_pipe = TextGenerationPipeline(model=llama_model, tokenizer=tokenizer)


# 데이터 로딩
# CSV 로딩 + 열 이름 지정
df = pd.read_csv("2_preposing.csv", header=None)
df.rename(columns={
    df.columns[1]: "discourse",
    df.columns[2]: "sentence"
}, inplace=True)

# 점수 저장 열 추가
for model in ["llama3"]:
    df[f"score_{model}"] = ""

# 프롬프트 생성 함수
def make_prompt(discourse, sentence):
    return f"""
Discourse: {discourse}
Sentence: {sentence}

How acceptable is the sentence as a continuation of the discourse?
Please respond with a number from 1 (not acceptable) to 7 (completely acceptable).
Answer:"""

def query_llama(prompt):
    inputs = tokenizer(prompt, return_tensors="pt")
    inputs = {k: v.to(llama_model.device) for k, v in inputs.items()}  # ✅ 여기!

    outputs = llama_model.generate(
        **inputs,
        max_new_tokens=10,
        do_sample=False,
        pad_token_id=tokenizer.eos_token_id
    )

    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return generated.replace(prompt, "").strip()



n_runs = 10

# 결과 저장용 리스트
results = []

# 모델별 평가 함수
for idx, row in tqdm(df.iterrows(), total=len(df), desc="Evaluating rows"):
    prompt = make_prompt(row["discourse"], row["sentence"])

    for model_name, query_func in {
        "llama3": query_llama
    }.items():
        for run in range(1, n_runs + 1):
            try:
                response = query_func(prompt)
                score = int("".join([c for c in response if c.isdigit()][:1]))
            except Exception as e:
                print(f"[{model_name}] Error on row {idx}, run {run}: {e}")
                score = ""

            # 각 응답을 새로운 행으로 저장
            results.append({
                "original_index": idx,
                "run": run,
                "model": model_name,
                "discourse": row["discourse"],
                "sentence": row["sentence"],
                "score": score
            })

            time.sleep(0.3)

# 결과를 DataFrame으로 변환
df_results = pd.DataFrame(results)

# 저장
df_results.to_csv("llama3_scores_expanded.csv", index=False)

from google.colab import files
files.download("llama3_scores_expanded.csv")



from google.colab import files
files.download("2_preposing_result.csv")

